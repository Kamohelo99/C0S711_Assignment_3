{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kamohelo99/C0S711_Assignment_3/blob/Ndumiso/supervised_Only_on_human_labeled_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bckr19dIDCgg"
      },
      "source": [
        "# Supervised Training Template\n",
        "\n",
        "This notebook outlines the steps required to train a convolutional neural network on the labelled MGCLS dataset.  It is not a complete solution; instead it provides guidance and placeholders for you to implement your own logic.  Follow the comments in each cell and fill in the `TODO` sections to build your own training pipeline.\n"
      ],
      "id": "bckr19dIDCgg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEZzbBBiDCgt"
      },
      "source": [
        "## 1. Set up environment\n",
        "\n",
        "Import the required libraries.  You may need to install some packages via pip if they are not already available in your environment.  Ensure you are using a GPU runtime if available."
      ],
      "id": "dEZzbBBiDCgt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePJ1cZVWDCgv"
      },
      "outputs": [],
      "source": [
        "# Install required libraries for this notebook\n",
        "!pip install -q iterative-stratification torchmetrics astropy\n",
        "\n",
        "# Standard Library Imports\n",
        "import os\n",
        "import re\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "\n",
        "# Core Data Science and ML Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from astropy.coordinates import SkyCoord\n",
        "from astropy import units as u\n",
        "\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
        "\n",
        "# Torchmetrics for Evaluation\n",
        "from torchmetrics import MetricCollection\n",
        "from torchmetrics.classification import MultilabelF1Score\n",
        "\n",
        "# Global Settings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "print(\"All required libraries are imported.\")\n"
      ],
      "id": "ePJ1cZVWDCgv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRQ8iprbDCg0"
      },
      "source": [
        "## 2. Define file paths\n",
        "\n",
        "Specify the locations of your extracted data and labels.  Update these variables to point to the directories on your own system or Colab environment."
      ],
      "id": "FRQ8iprbDCg0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPyosU58DCg2"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive to access your data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# TODO: set these paths appropriately, I save mine in MyDrive\n",
        "DRIVE_PATH = Path(\"/content/drive/MyDrive/assignmentdata\")\n",
        "DATA_DIR = DRIVE_PATH / \"data\"\n",
        "LABELS_FILE = DRIVE_PATH / \"labels.csv\"\n",
        "\n",
        "# directories for generated files\n",
        "CHECKPOINT_DIR = DRIVE_PATH / \"checkpoints\"\n",
        "SPLIT_DIR = DRIVE_PATH / \"splits\"\n",
        "DATA_DIR.mkdir(exist_ok=True)\n",
        "CHECKPOINT_DIR.mkdir(exist_ok=True)\n",
        "SPLIT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# We will define num_classes dynamically after inspecting the data.\n",
        "num_classes = None\n",
        "\n",
        "print(f\"Data root set to: {DATA_DIR}\")\n",
        "print(\"Please ensure your data (labels.csv, typ/, exo/) is in this directory.\")\n"
      ],
      "id": "oPyosU58DCg2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqTBoXk_DCg3"
      },
      "source": [
        "## 3. Load and inspect the labels\n",
        "\n",
        "Use pandas to read `labels.csv` and explore its columns.  Identify the coordinate columns (e.g. `ra`, `dec`) and the label columns (e.g. `label1`, `label2`, ...).  You will need this information when implementing the coordinate matching function."
      ],
      "id": "mqTBoXk_DCg3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4gIhAtRDCg5"
      },
      "outputs": [],
      "source": [
        "# Read the labels CSV, providing column names as the file has no header\n",
        "labels_df = pd.read_csv(LABELS_FILE, names=['RA', 'DEC', 'L1', 'L2', 'L3', 'L4'])\n",
        "\n",
        "# Consolidate all label columns into a single 'labels' string\n",
        "label_cols = ['L1', 'L2', 'L3', 'L4']\n",
        "labels_df['labels'] = labels_df[label_cols].apply(lambda row: ', '.join(row.dropna().astype(str)), axis=1)\n",
        "\n",
        "print(\"Labels DataFrame:\")\n",
        "labels_df.head()\n"
      ],
      "id": "v4gIhAtRDCg5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DirnG2LEDCg6"
      },
      "source": [
        "## 4. Implement coordinate parsing and label matching\n",
        "\n",
        "The dataset module in `src/dataset.py` provides skeleton functions `parse_coords_from_filename()` and `match_labels()`.  You need to implement these functions so they correctly extract coordinates from image filenames and find the nearest label entry.  Test your implementation in this cell.  For example, pick a few filenames from the `typ` directory and check that the returned labels make sense."
      ],
      "id": "DirnG2LEDCg6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e46b382",
      "metadata": {
        "id": "8e46b382"
      },
      "outputs": [],
      "source": [
        "# combine the logic for coordinate parsing and matching.\n",
        "def extract_coords_from_filename(fname: str):\n",
        "    \"\"\"Extract RA/Dec from filename using a robust regex.\"\"\"\n",
        "    pattern = r\"([-+]?\\d*\\.\\d+|\\d+)\\s+([-+]?\\d*\\.\\d+|\\d+)_\"\n",
        "    m = re.search(pattern, fname)\n",
        "    if m:\n",
        "        try:\n",
        "            return float(m.group(1)), float(m.group(2))\n",
        "        except (ValueError, IndexError):\n",
        "            return None, None\n",
        "    return None, None\n",
        "\n",
        "def perform_matching(data_dir, labels_df):\n",
        "    \"\"\"Scans image folders, extracts coordinates, and matches them to labels using Astropy.\"\"\"\n",
        "    print(\"--- Starting Coordinate Matching ---\")\n",
        "\n",
        "    # Create an Astropy SkyCoord object for the catalog labels for fast matching\n",
        "    catalog = SkyCoord(ra=labels_df[\"RA\"].values*u.deg, dec=labels_df[\"DEC\"].values*u.deg, frame='icrs')\n",
        "\n",
        "    # Scan image folders ('typ' and 'exo')\n",
        "    imgs = []\n",
        "    for folder in [\"typ/typ_PNG\", \"exo/exo_PNG\"]:\n",
        "        folder_path = data_dir / folder\n",
        "        if not folder_path.exists(): continue\n",
        "        for fpath in folder_path.glob(\"*.png\"):\n",
        "            ra, dec = extract_coords_from_filename(fpath.name)\n",
        "            if ra is not None:\n",
        "                imgs.append({\"image_path\": str(fpath), \"RA_img\": ra, \"DEC_img\": dec})\n",
        "\n",
        "    images_df = pd.DataFrame(imgs)\n",
        "    print(f\"Found {len(images_df)} PNG files with valid coordinates.\")\n",
        "\n",
        "    # Use Astropy to find the nearest neighbor in the catalog for each image\n",
        "    image_coords = SkyCoord(ra=images_df[\"RA_img\"].values*u.deg, dec=images_df[\"DEC_img\"].values*u.deg, frame='icrs')\n",
        "    idx, sep2d, _ = image_coords.match_to_catalog_sky(catalog)\n",
        "\n",
        "    # Combine the matched data into a single DataFrame\n",
        "    matched_labels = labels_df.iloc[idx].reset_index(drop=True)\n",
        "    combined_df = pd.concat([images_df, matched_labels], axis=1)\n",
        "    combined_df['distance_arcsec'] = sep2d.arcsec\n",
        "\n",
        "    print(\"Matching complete.\")\n",
        "    return combined_df\n",
        "\n",
        "# Execute the matching process\n",
        "combined_data_df = perform_matching(DRIVE_PATH, labels_df)\n",
        "\n",
        "print(\"\\n--- Matched Data Sample ---\")\n",
        "combined_data_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9gkonnuDChB"
      },
      "source": [
        "## 5. Create the dataset and dataloaders\n",
        "\n",
        "Here we instantiate the `RadioDataset` for the labelled images.  You may choose to combine the typical and exotic datasets or create separate datasets and use `ConcatDataset`.  Apply appropriate transformations (e.g. resizing, normalisation, augmentation)."
      ],
      "id": "G9gkonnuDChB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zN3pWlfYDChC"
      },
      "outputs": [],
      "source": [
        "# Define Transformations\n",
        "IMG_SIZE = 128 # Using the size from Kamo\n",
        "train_tf = transforms.Compose([\n",
        "    transforms.RandomRotation(360),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomVerticalFlip(p=0.5),\n",
        "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8, 1.0)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5]),\n",
        "])\n",
        "val_tf = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5]),\n",
        "])\n",
        "\n",
        "# Create Stratified Splits\n",
        "print(\"--- Creating Multilabel Stratified Splits ---\")\n",
        "combined_data_df[\"labels_list\"] = combined_data_df[\"labels\"].astype(str).apply(\n",
        "    lambda s: [lbl.strip() for lbl in s.split(\",\") if lbl.strip()]\n",
        ")\n",
        "mlb = MultiLabelBinarizer()\n",
        "y = mlb.fit_transform(combined_data_df[\"labels_list\"])\n",
        "CLASSES = mlb.classes_.tolist()\n",
        "num_classes = len(CLASSES)\n",
        "print(f\"Found {num_classes} unique classes: {CLASSES}\")\n",
        "\n",
        "# 70/30 split for train and validation\n",
        "msss = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=SEED)\n",
        "train_idx, val_idx = next(msss.split(combined_data_df, y))\n",
        "\n",
        "train_df = combined_data_df.iloc[train_idx]\n",
        "val_df = combined_data_df.iloc[val_idx]\n",
        "print(f\"Split complete. Train size: {len(train_df)}, Validation size: {len(val_df)}\")\n",
        "\n",
        "# Define the Dataset Class\n",
        "class RadioDataset(Dataset):\n",
        "    def __init__(self, df, classes, transform=None):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.transform = transform\n",
        "        self.mlb = MultiLabelBinarizer(classes=classes)\n",
        "    def __len__(self): return len(self.df)\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img = Image.open(row['image_path']).convert('L') # Force 1-channel\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        labels_one_hot = self.mlb.fit_transform([row['labels_list']])[0]\n",
        "        labels = torch.tensor(labels_one_hot, dtype=torch.float32)\n",
        "        return img, labels\n",
        "\n",
        "# Instantiate Datasets and DataLoaders\n",
        "train_dataset = RadioDataset(train_df, classes=CLASSES, transform=train_tf)\n",
        "val_dataset = RadioDataset(val_df, classes=CLASSES, transform=val_tf)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "print(\"Datasets and DataLoaders are ready.\")"
      ],
      "id": "zN3pWlfYDChC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXlfW4HMDChD"
      },
      "source": [
        "## 6. Build the model\n",
        "\n",
        "Create a ResNet‑based classifier using the helper function `build_model()` in `src/model.py`.  Remember to pass `num_classes` equal to the total number of labels you have.  Move the model to GPU if available."
      ],
      "id": "DXlfW4HMDChD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zVqChRgDChE"
      },
      "outputs": [],
      "source": [
        "def build_adapted_model(model_name=\"efficientnet_b0\", num_classes=num_classes):\n",
        "    \"\"\"Adapts a pre-trained model for 1-channel input and our classification task.\"\"\"\n",
        "    model = models.get_model(model_name, weights='IMAGENET1K_V1')\n",
        "\n",
        "    # Adapt first conv layer for 1-channel input\n",
        "    conv_layer = model.features[0][0]\n",
        "    new_conv = nn.Conv2d(1, conv_layer.out_channels,\n",
        "                         kernel_size=conv_layer.kernel_size, stride=conv_layer.stride,\n",
        "                         padding=conv_layer.padding, bias=conv_layer.bias is not None)\n",
        "    new_conv.weight.data = conv_layer.weight.data.mean(dim=1, keepdim=True)\n",
        "    model.features[0][0] = new_conv\n",
        "\n",
        "    # Adapt final classifier\n",
        "    in_features = model.classifier[1].in_features\n",
        "    model.classifier = nn.Sequential(nn.Dropout(p=0.3), nn.Linear(in_features, num_classes))\n",
        "\n",
        "    print(f\"Adapted {model_name} for 1-channel input and {num_classes} classes.\")\n",
        "    return model\n",
        "\n",
        "# Instantiate the model\n",
        "model = build_adapted_model(num_classes=num_classes)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "print(f\"Model moved to device: {device}\")\n",
        "\n",
        "# Define loss function and optimiser\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n"
      ],
      "id": "-zVqChRgDChE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2IWQPRnDChF"
      },
      "source": [
        "## 7. Training loop\n",
        "\n",
        "Implement the training loop.  For each batch, convert the list of label strings into a multi‑hot tensor.  Compute the loss, backpropagate, and update the model weights.  At the end of each epoch, evaluate the model on the validation set and compute metrics such as precision, recall, F1 and mAP using functions from `src/utils.py`.  Save the best model checkpoint."
      ],
      "id": "M2IWQPRnDChF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhaF2ReADChG"
      },
      "outputs": [],
      "source": [
        "# Define the evaluation loop logic\n",
        "def evaluate_epoch(model, loader, device, metrics_collection):\n",
        "    model.eval()\n",
        "    metrics_collection.reset()\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            metrics_collection.update(outputs, labels.int())\n",
        "    return metrics_collection.compute()\n",
        "\n",
        "# Main Training Loop\n",
        "num_epochs = 50\n",
        "best_f1 = 0.0\n",
        "metrics = MetricCollection({'MacroF1': MultilabelF1Score(num_labels=num_classes, average='macro')}).to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    for images, labels in pbar:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        pbar.set_postfix(loss=running_loss/len(pbar))\n",
        "\n",
        "    # Validation\n",
        "    val_metrics = evaluate_epoch(model, val_loader, device, metrics)\n",
        "    val_f1 = val_metrics['MacroF1'].item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1} Summary: Train Loss: {running_loss/len(train_loader):.4f}, Val MacroF1: {val_f1:.4f}\")\n",
        "\n",
        "    # Save best model\n",
        "    if val_f1 > best_f1:\n",
        "        best_f1 = val_f1\n",
        "        torch.save(model.state_dict(), CHECKPOINT_DIR / 'supervised_best_model.pth')\n",
        "        print(f'New best model saved with F1-score: {best_f1:.4f}')"
      ],
      "id": "DhaF2ReADChG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdEvawhJDChI"
      },
      "source": [
        "## 8. Save the model\n",
        "\n",
        "After training, save your model checkpoint to disk.  You can use this checkpoint in the semi‑supervised phase."
      ],
      "id": "KdEvawhJDChI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Y2xpAGSDChI"
      },
      "outputs": [],
      "source": [
        "# The training loop already saves the BEST model.\n",
        "# This cell will save the FINAL model after the last epoch, for comparison.\n",
        "final_model_path = CHECKPOINT_DIR / 'supervised_final_model.pth'\n",
        "torch.save(model.state_dict(), final_model_path)\n",
        "print(f\"Training finished. Final model state saved to: {final_model_path}\")\n",
        "print(f\"The best performing model was saved to: {CHECKPOINT_DIR / 'supervised_best_model.pth'}\")\n"
      ],
      "id": "4Y2xpAGSDChI"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}