{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Training Template\n",
    "\n",
    "This notebook outlines the steps required to train a convolutional neural network on the labelled MGCLS dataset.  It is not a complete solution; instead it provides guidance and placeholders for you to implement your own logic.  Follow the comments in each cell and fill in the `TODO` sections to build your own training pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set up environment\n",
    "\n",
    "Import the required libraries.  You may need to install some packages via pip if they are not already available in your environment.  Ensure you are using a GPU runtime if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: import modules\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Import your own modules from the `src` folder (adjust sys.path as needed)\n",
    "import sys\n",
    "sys.path.append('..')  # adjust this path if running from a different directory\n",
    "from src.dataset import RadioDataset, parse_coords_from_filename, match_labels\n",
    "from src.model import build_model\n",
    "from src.utils import compute_f1, compute_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define file paths\n",
    "\n",
    "Specify the locations of your extracted data and labels.  Update these variables to point to the directories on your own system or Colab environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: set these paths appropriately\n",
    "data_root = '/content/data'   # directory containing 'typ' and 'exo' subdirectories\n",
    "labels_csv = '/content/labels.csv'\n",
    "\n",
    "# Define the number of classes (you must supply this based on your label set)\n",
    "num_classes = 8  # example number; adjust to your dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and inspect the labels\n",
    "\n",
    "Use pandas to read `labels.csv` and explore its columns.  Identify the coordinate columns (e.g. `ra`, `dec`) and the label columns (e.g. `label1`, `label2`, ...).  You will need this information when implementing the coordinate matching function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the labels CSV\n",
    "labels_df = pd.read_csv(labels_csv)\n",
    "labels_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implement coordinate parsing and label matching\n",
    "\n",
    "The dataset module in `src/dataset.py` provides skeleton functions `parse_coords_from_filename()` and `match_labels()`.  You need to implement these functions so they correctly extract coordinates from image filenames and find the nearest label entry.  Test your implementation in this cell.  For example, pick a few filenames from the `typ` directory and check that the returned labels make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e46b382",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_coords_from_filename(filename):\n",
    "    \"\"\"\n",
    "    Extract RA and Dec from MGCLS filename, e.g., 'J123456.78-321456.7.png'\n",
    "    Returns: tuple of floats (ra_deg, dec_deg)\n",
    "    \"\"\"\n",
    "    # Strip extension\n",
    "    basename = filename.split('.')[0] if '.' in filename else filename\n",
    "    match = re.match(r'J(\\d{6}\\.?\\d*)([+-]\\d{6}\\.?\\d*)', basename)\n",
    "    if not match:\n",
    "        raise ValueError(f\"Filename format not recognized: {filename}\")\n",
    "\n",
    "    ra_str, dec_str = match.groups()\n",
    "    ra_deg = hms_string_to_deg(ra_str)\n",
    "    dec_deg = dms_string_to_deg(dec_str)\n",
    "    return ra_deg, dec_deg\n",
    "\n",
    "def hms_string_to_deg(hms_str):\n",
    "    \"\"\"Convert RA from hhmmss.ss string to degrees\"\"\"\n",
    "    h = int(hms_str[0:2])\n",
    "    m = int(hms_str[2:4])\n",
    "    s = float(hms_str[4:])\n",
    "    return (h + m / 60.0 + s / 3600.0) * 15.0\n",
    "\n",
    "def dms_string_to_deg(dms_str):\n",
    "    \"\"\"Convert Dec from ddmmss.ss string to degrees\"\"\"\n",
    "    sign = -1 if dms_str.startswith('-') else 1\n",
    "    dms_str = dms_str[1:] if dms_str[0] in '+-' else dms_str\n",
    "    d = int(dms_str[0:2])\n",
    "    m = int(dms_str[2:4])\n",
    "    s = float(dms_str[4:])\n",
    "    return sign * (d + m / 60.0 + s / 3600.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a605d995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_labels(coords, labels_df, tol_arcsec=1.0):\n",
    "    \"\"\"\n",
    "    Match image coords (RA, Dec) to a row in labels_df using a tolerance in arcseconds.\n",
    "    Returns: list of class label strings\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    ra, dec = coords\n",
    "    # Convert to numpy for vectorized diff\n",
    "    ra_diff = np.abs(labels_df['ra'].values - ra)\n",
    "    dec_diff = np.abs(labels_df['dec'].values - dec)\n",
    "\n",
    "    # Angular tolerance in degrees\n",
    "    tol_deg = tol_arcsec / 3600.0\n",
    "\n",
    "    matches = (ra_diff < tol_deg) & (dec_diff < tol_deg)\n",
    "    if not matches.any():\n",
    "        return []\n",
    "\n",
    "    matched_row = labels_df[matches].iloc[0]\n",
    "    label_str = matched_row['labels']  # assumed comma-separated string\n",
    "    return [l.strip() for l in label_str.split(',') if l.strip()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labels.csv\n",
    "labels_df = pd.read_csv('data/labels.csv')\n",
    "\n",
    "# Test with a file from 'typ/'\n",
    "filename = os.listdir('data/typ')[0]\n",
    "coords = parse_coords_from_filename(filename)\n",
    "labels = match_labels(coords, labels_df)\n",
    "\n",
    "print(\"Filename:\", filename)\n",
    "print(\"Coordinates:\", coords)\n",
    "print(\"Labels:\", labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create the dataset and dataloaders\n",
    "\n",
    "Here we instantiate the `RadioDataset` for the labelled images.  You may choose to combine the typical and exotic datasets or create separate datasets and use `ConcatDataset`.  Apply appropriate transformations (e.g. resizing, normalisation, augmentation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: define transformations and instantiate the dataset\n",
    "transform = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "# Instantiate dataset (you may need to implement label conversion to multi‑hot here)\n",
    "dataset = RadioDataset(image_root=os.path.join(data_root, 'typ'), labels_csv=labels_csv, transform=transform, label_df=labels_df)\n",
    "\n",
    "# include exotic images:\n",
    "exo_dataset = RadioDataset(image_root=os.path.join(data_root, 'exo'), labels_csv=labels_csv, transform=transform, label_df=labels_df)\n",
    "from torch.utils.data import ConcatDataset\n",
    "dataset = ConcatDataset([dataset, exo_dataset])\n",
    "\n",
    "# Split into training and validation sets\n",
    "val_size = int(0.2 * len(dataset))\n",
    "train_size = len(dataset) - val_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Build the model\n",
    "\n",
    "Create a ResNet‑based classifier using the helper function `build_model()` in `src/model.py`.  Remember to pass `num_classes` equal to the total number of labels you have.  Move the model to GPU if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = build_model(num_classes=num_classes, pretrained=False)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss function and optimiser\n",
    "criterion = torch.nn.BCEWithLogitsLoss()  # consider pos_weight for class imbalance\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training loop\n",
    "\n",
    "Implement the training loop.  For each batch, convert the list of label strings into a multi‑hot tensor.  Compute the loss, backpropagate, and update the model weights.  At the end of each epoch, evaluate the model on the validation set and compute metrics such as precision, recall, F1 and mAP using functions from `src/utils.py`.  Save the best model checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudocode for training loop (fill in the missing parts)\n",
    "num_epochs = 10\n",
    "best_f1 = 0.0\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        images = batch['image'].to(device)\n",
    "        # TODO: convert batch['label'] (list of strings) into a multi‑hot tensor of shape (batch_size, num_classes)\n",
    "        # labels_tensor = ...\n",
    "        labels_tensor = torch.zeros(len(images), num_classes)  # placeholder\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels_tensor.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "    train_loss = running_loss / len(train_dataset)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_scores = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            images = batch['image'].to(device)\n",
    "            # TODO: convert labels\n",
    "            labels_tensor = torch.zeros(len(images), num_classes)  # placeholder\n",
    "            outputs = model(images)\n",
    "            y_true.append(labels_tensor.numpy())\n",
    "            y_scores.append(torch.sigmoid(outputs).cpu().numpy())\n",
    "    y_true_arr = np.concatenate(y_true, axis=0)\n",
    "    y_scores_arr = np.concatenate(y_scores, axis=0)\n",
    "    precision, recall, f1 = compute_f1(y_true_arr, y_scores_arr)\n",
    "    mAP = compute_map(y_true_arr, y_scores_arr)\n",
    "    print(f'Epoch {epoch+1}: loss={train_loss:.4f}, f1={f1:.4f}, mAP={mAP:.4f}')\n",
    "    # TODO: save best model\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        # torch.save(model.state_dict(), 'best_model.pth')\n",
    "        print('New best model saved')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save the model\n",
    "\n",
    "After training, save your model checkpoint to disk.  You can use this checkpoint in the semi‑supervised phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the trained model\n",
    "torch.save(model.state_dict(), 'resnet18_supervised.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
